\section{Insights of MLCommons in Education}
\label{sec:edu-mlcommons-insights}

The MLCommons benchmarks provide a valuable starting point for
educational material addressing various aspects of the machine and
deep learning ecosystem. This includes benchmarks targeted to a
variety of system resources from tiny devices to the largest High
Performance Computing and Data Systems in the world, while being able
to adapt and test them on platforms in-between these two
extremes. Thus they become ideal targets for adaptation in AI classes
that want to go beyond typical starting applications such as MNIST.

We have gained practical experience while adapting benchmarks from the
MLCommons Science Working group while collaborating with various
universities and student groups from the University of Virginia, New
York University, and Indiana University. Furthermore, it was used in
undergraduate education that was continued from FAMU as an REU and is
now executed at the University of Virginia as research activity by a
past student from the REU. We found that the examples provide value
for classes, capstones, Research Experiences for Undergraduates (REU),
team project-oriented software engineering and computer science
classes, and internships.

We observed that while traditional classes limit their resource needs
and thus the target application to a very short period so assignments
can be conducted in a short period, some MLCommons benchmarks go well
beyond this while confronting the students not only with
the theoretical background of the ML algorithm, but also with the Big Data
Systems on which such benchmarks need to be executed due to the
complexity and time requirements of some of the benchmarks. This is
especially the case when hyperparameters are identified to derive more
scientific accurate examples. It also allows the students to explore
different algorithms applied to these problems.

From our experiences with these various efforts, we found that the
following lessons provided significant add-on learning experiences:


\begin{itemize}


  \item {\bf Teamwork.} Students benefit from focusing on the success
    and collaboration of the entire team rather than mere
    individualism, as after graduation students may work in large
    teams. This includes the opportunity for pair programming, but also
    the fact that careful time planning in the team is needed to
    succeed.  This also includes how to collaborate with peers using
    professional, industry-standard coding software and management of
    code in a team through the Git version control system.

  \item {\bf Interdisciplinary Research.} Many of the applications in
    MLCommons are requiring interdisciplinary research between the domain
    scientists, ML experts, and IT engineers. As part of the teamwork
    students have the opportunity to participate not only in their
    discipline, but learn about how to operate in an interdisciplinary
    team.

  \item {\bf System Benchmarking vs. Science Benchmarking.} Students
    can learn about two different benchmarking efforts. The first is
    system-level benchmarking in which a system is compared based
    on a predefined algorithm and its parameters. The second is the
    benchmarking of a scientific algorithm in which the quality of the
    algorithm is compared with each other.

  \item {\bf Software ecosystem.} Students are often only using a
    pre-setup environment prepared explicitly for a class that makes
    management for the class easier, but does not expose the students
    to various ways of setting up and utilizing the large variety of
    software related to big data systems. This includes setting up
    Python beyond the use of conda and Colab notebooks, the use of
    queueing systems, containers, and cloud computing software for
    AI, DL, and HPC experiments as well as other advanced aspects of
    software engeneering.

  \item {\bf Execution ecosystem.} While in-class problems typically
    do not require as many computing resources, some of the examples in
    MLCommons require a significant organizational aspect to select
    and run meaningful calculations that enhance the accuracy of the
    results. Careful planning with workflows and the potential use
    of hybrid heterogeneous systems significantly improves the
    awareness to deal with not only the laptop but also the large
    available resources students may get access to while leveraging
    leadership-class computing resources, or their own local HPC
    system when available. This includes dealing with system policies,
    remote system access, and frugal planning of experiments through
    prediction of runtimes and planning of hyperparameter searches. This
    also can include dealing with energy consumption and other
    environmental parameters.

  \item {\bf Parallelization.} The examples provide a basis for
    learning about various parallelization aspects. This includes the
    parallelization on the job level and hyperparameters searches, but
    also on the use of parallelization methods provided by large-scale
    GPU bases big data systems.

  \item {\bf IO Data management.} One other important lesson is the
    efficient and effective use of data stores to execute for example
    DL algorithms that require a large number of fast IO interactions.

  \item {\bf Data Analysis.} The examples provide valuable input to
    further, enhance abilities to conduct non-trivial data analysis
    through advanced Python scripts while integrating them in
    coordinated runs to analyze log files that are created to
    validate the numerical stability of the benchmarks. This obviously
    includes the utilization of popular data analysis libraries (such
    as pandas) as well as vizualization. It also allows students to
    focus on identifying a result that can be communicated in a
    professional manner as the next point illustrates.


  \item {\bf Professional and Academic Communication.} The results
    achieved need to be communicated to a larger audience and the
    students can engage in a report, paper, and
    presentation writing opportunities addressing scientific and
    professional communities.

  \item {\bf Benefits to Society.} The MLCommons benchmarks are
    including opportunities to improve the quality of ML algorithms
    that can be applied to societal tasks. Obviously, improving
    benchmarks such as earthquake forecasting are beneficial to the
    society and can motivate students to participate in such
    educational opportunities.

\end{itemize}

