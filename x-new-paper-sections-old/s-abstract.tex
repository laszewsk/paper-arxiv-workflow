
\begin{abstract}

  \section{}


    Over many decades, High Performance Computing systems have been made available to the research community through research organizations and also recently made available through commercially available cloud solutions. The use of such systems has traditionally been restrictive due to the high costs of computing resources as well as the complex software to offer them efficiently to the community. Over time, we have also seen the utilization of federated resources such as Grids, Clouds, and today's hyperscale data centers. Still, despite the many software systems and frameworks in support of these resource infrastructures, their utilization has been a challenge, especially in the academic community educating the next generation of scientists. We also found that using limited benchmarks on various machines, even if they are not federated, pose a significant hurdle in demonstrating compute resource capability to the many communities with their highly varied computational needs. While popular frameworks such as Gateways and, on the other spectrum, Jupyter notebooks promise usage simplification, it is even more important that a wide variety of scientific benchmarks are available that outline how such machines can be best utilized and align with the scientists application-specific computational challenge. We found that this is best done in the form of workflow templates that are designed for a specific problem and can be adapted to a specific scientific application.

  Within this paper, we focus first on identifying common usage patterns that outline which workflow templates we have found most useful based on our experiences over many decades. Naturally, there are many other patterns available that may be addressed by other frameworks. However, we focus here on templates that have been used by us, based on decades of use of HPC systems dating back to the early parallel computers. Recently, we have enhanced and expanded our experience by participating in  the MLCommons Science working group. We found that focusing on simple tools addressing what we call {\em experiment management} as part of the the more general computational workflow improves adaptability in the  educational community. Hence, they can become valuable aspects into what we term \textit{benchmark carpentry}.

We have verified this approach based on the experiences of two independently developed software tools and frameworks that upon closer inspection provide a large amount of overlap in functionality. These systems are the Experiment Executor which is part of a larger bag of services distributed as part of Cloudmesh that has been used for more than a decade, as well as SmartSim developed by Hewlett Packard Enterprise which similarly addresses experiment management. These frameworks have been tested on various scientific applications. In our earlier work, this was done on two scientific applications: conduction cloudmask and earthquake prediction. More recently this work was extended to include experiment executions that involve the interaction of simulation and AI/ML. Lastly, we focus on how these frameworks are also simplifying the development of a surrogate for computational fluid dynamics.


\tiny \keyFont{ \section{Keywords:} deep learning, benchmarking, hyper
  parameter search, hybrid heterogeneous hyperparameter search,
  scientific benchmark, Cloudmesh, SmartSim}

% All article types: you may provide up to 8 keywords; at least 5 are mandatory.

\end{abstract}
