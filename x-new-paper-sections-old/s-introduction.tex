
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Benchmarks are useful for comparing the performance of computing resources and their suitability for applications that may be executed with them. In particular, application users can benefit when benchmarks have analogues with their applications allowing them to assess and potentially predict feasibility of their expected scientific workloads. Typical benchmarks include measuring the performance of CPUs, GPUs, data storage, data transfer, and energy consumption.

The most challenging application problems require HPC-scale computing resources. These applications are influenced by machine-specific performance, but it is often not clear how a specific application performs when benchmarks may not relate closely enough to the applications. Furthermore, the shared nature of systems with hundreds of users  adds another dimension of complexity: the execution of the benchmark may differ if deployed on hardware reserved for a single user versus shared hardware and in cases with multi-step jobs, scheduling on crowded machines introduces additional delay. These issues make predicting real-time end-to-end performance benchmarking very challenging. Hence, in many systems a benchmark is run as a single user and the queue wait time is often ignored.

Arguably, Linpack performance is the most well-known HPC benchmarks forming the basis for the published list of the top 500 HPC machines~\citep{www-top500}. Recently, a benchmark using High-Performance Conjugate Gradient (HPCG) has been added to complement the Linpack benchmark so that an additional understanding of the machine's performance can be achieved~\citep{www-top500} through a different application. With increasing awareness of the electrical requirements for these machines, the energy consumption of watts per flop is the figure of merit for the Green500 benchmark~\citep{green500}.

From such benchmarks, one can only derive the {\em potential} of the entire HPC machine, whereas application users need to develop their own benchmark experiments representative of the application's needs. These benchmarks are often run at smaller scales and introduce scaling factors based on theoretical assumptions to then predict the performance of larger problems. In some cases, the application needs to be rewritten to fit within the constraints of available resources and compute time to access them. In other cases, it is not the hardware of the machine that leads to lower-than-expected performance, but logistical policies. For example, HPC platforms usually have scheduling policies that maximize overall machine utilization while allowing a balanced number of jobs for users. While the HPC policies can be changed for special circumstances, it is often not a permanent option because the individual benchmark user impacts negatively the larger user community. Therefore, realistic application benchmarks often need to distinguish between performance based on single-user optimal policies vs. production policies. 

The increasing using of machine learning has led to the development of benchmarks focused on training and inference from neural networks. The MLCommons group tries to make the use of machine learning easier while creating typical benchmarks for supercomputers. While raw performance is measured by most working groups in MLCommons measuring the performance of many well-known AI applications, the science working group also tries to identify the best algorithms or even data sets to obtain benchmarks based not only on performance but also on the accuracy of the problem solution.

Clearly, these multiple objectives preclude the use of a single benchmark, but rather require many different experiments with potentially different hyperparameters and datasets. Setting up a workflow that supports such experiments is often complex especially if they exceed the center's policies and need to be modified accordingly. Therefore, the capability of coordinating many experiments, their workflows, and aggregating the results is key to this type of benchmark while staying within the limits associated with the HPC user defined by the datacenter.

Throughout the paper we use the general term {\em computational workflow} or simply {\em workflow} to indicate the chain of tasks that are needed to produce from a set of inputs the outputs \citep{def-workflow} as has been used for decades. To distinguish separately executed workflows that may lead to a set of very distinctive results we use the term experiment execution. To be more specific, we use the term {\bf\em experiment execution} to indicate a specific workflow that includes the execution of a benchmark experiment while considering the provisioning of data, the execution of the algorithm on the data to achieve a result, and the variation of the hyperparameters as part of the many different single executions run on the infrastructure to obtain the result. We distinguish this very specific definition of this workflow from the rather overloaded term of {\em workflow} by many different communities. As our focus is the experiment execution as part of benchmarks, we also use the term {\bf\em Experiment Executor} in this paper referring to the execution engine to conduct such experiments. The coordination of computational task placed on the available compute infrastructure including different sites we term {\bf\em compute coordinator}. The different terms have been introduced as the task of benchmarking includes a pipeline often with repeated executions. 

This is subset of a general workflows, which by many in the community have been 
predominantly used to express them as direct acyclic graphs (DAGs). However for our experiments it is essential that we have an easy way to integrate multidimensional loops iterating over hyperparameters, which are much easier to understand and formulate than DAGs. Nevertheless, we obviously also need to support DAGs and not only do iterations.

The complexity of the compute infrastructure and the applications requires that workflow toolkits provide sufficient flexibility to support the experiment execution. Thus we need to support a bottom-up approach as well as a top-down approach. Through the bottom-up approach we need APIs, components and scripts that can be adapted by integrating new applications. In addition the top-down approach also allows the conceptual integration of multiple experiments run on various compute resources including those hosted on different sites. The results of these experiments is then consolidated and a consistent  eport can be generated from them while promoting Open Science and the FAIR Principles \citep{wilkinson2016fair}. We also need to be able to support a top-down approach where we learn from the application users what features their benchmarks need to include and be able to utilize lower-level libraries to support them.

Two software libraries Cloudmesh and SmartSim were developed indepently and contains apsects of these bottom-up and top-down approaches while offering similar functionality. This gives us the confidence that what we describe here has general applicability and is useful to the community. Both provide Python-based libraries used to describe the components of an experiment execution. While Cloudmesh also provides a compute coordinator to integrate heterogeneous experiments, SmartSim allows users to deploy an in-memory specialized datastore which is also capable of performing AI inference and exchange data between components of the workflow. In Cloudmesh \citep{cloudmesh-ee} community-developed reusable tools can be used for this.

The paper is structured as follows. After a brief introduction in Section \ref{sec:introduction}, we outline some requirements in Section \ref{sec:requirements} that we found important as part of our activities in the area motivated by hardware, user, software, and data management requirements. In  Section \ref{sec:executors} we focus on presentation an overview of two independent implementations addressing many of the requirements presented, namely SmartSim and Cloudmesh Experiment Executor and Compute Coordinator. The overview includes also a comparison between these systems and provide evidence how the requirements we identified are fulfilled by them. Furthermore, we present a recent extension to our work making it possible to utilize cloud resources.
We have tested the system on use cases that we very briefly list in Section \ref{sec:use_cases}.
To position our work we also added a related work Section \ref{sec:related}.
 Finally, we conclude the paper with Section \ref{sec:conclusion}.

\begin{comment}
In an appendix, we also summarize relevant research done by the coauthors of this paper that addresses many issues related to workflows and are directly related to activities the authors have been involved with (Section \ref{sec:related}).
\end{comment}